{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l0ISwVoP0ln"
      },
      "source": [
        "# Assistive Gym - New environment for multi robots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9pLY4c_XHya"
      },
      "source": [
        "## Create the environment\n",
        "\n",
        "Let's create a new environment where the goal is for two robots each move its end effector to a person's mouth.\n",
        "\n",
        "This environment will support all existing robots in Assistive Gym and also support a static human motion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pybullet as p\n",
        "from gym import spaces\n",
        "\n",
        "from assistive_gym.envs.env import AssistiveEnv\n",
        "\n",
        "class ReachingMultiRobotEnv(AssistiveEnv):\n",
        "    def __init__(self, robot_1, human, robot_2):\n",
        "        # We will call this the 'feeding' task so we don't have to redefine robot positions/orientations in the agent/sawyer.py and agent/panda.py files.\n",
        "        obs_robot_1_len =(17 + len(robot_1.controllable_joint_indices) - (len(robot_1.wheel_joint_indices) if robot_1.mobile else 0))\n",
        "        obs_robot_2_len =(17 + len(robot_2.controllable_joint_indices) - (len(robot_2.wheel_joint_indices) if robot_2.mobile else 0))\n",
        "        super(ReachingMultiRobotEnv, self).__init__(robot=robot_1, human=human, task='feeding',\n",
        "                                          obs_robot_len=(obs_robot_1_len + obs_robot_2_len), \n",
        "                                          obs_human_len=(18 + len(human.controllable_joint_indices)))\n",
        "\n",
        "        #update with robot actions space \n",
        "        self.robot_1 = robot_1\n",
        "        self.robot_2 = robot_2\n",
        "        \n",
        "        self.obs_robot_1_len = obs_robot_1_len\n",
        "        self.obs_robot_2_len = obs_robot_2_len\n",
        "        \n",
        "        #update action space\n",
        "        self.action_robot_1_len = len(robot_1.controllable_joint_indices) if robot_1 is not None else 0\n",
        "        self.action_robot_2_len = len(robot_2.controllable_joint_indices) if robot_2 is not None else 0\n",
        "        self.action_robot_len = self.action_robot_1_len + self.action_robot_2_len\n",
        "        self.action_space = spaces.Box(low=np.array([-1.0]*(self.action_robot_len+self.action_human_len), dtype=np.float32), \n",
        "                                        high=np.array([1.0]*(self.action_robot_len+self.action_human_len), dtype=np.float32), \n",
        "                                        dtype=np.float32)\n",
        "        self.action_space_robot = spaces.Box(low=np.array([-1.0]*self.action_robot_len, dtype=np.float32), \n",
        "                                        high=np.array([1.0]*self.action_robot_len, dtype=np.float32), \n",
        "                                        dtype=np.float32)\n",
        "\n",
        "    def get_obs(self):\n",
        "        return {'robot_1':self._get_obs(agent='robot_1'),\n",
        "                'robot_2':self._get_obs(agent='robot_2'),\n",
        "                'robot':self._get_obs(agent='robot'),\n",
        "                'human':self._get_human_obs()}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.human.controllable:\n",
        "            #map the agent order : robot_1 human robot_2\n",
        "            robot_1_action = action['robot'][0:self.action_robot_1_len]\n",
        "            robot_2_action = action['robot'][self.action_robot_1_len:self.action_robot_1_len+self.action_robot_2_len]\n",
        "            action = np.concatenate([robot_1_action, action['human'],robot_2_action])\n",
        "        # Execute the action. Step the simulator forward\n",
        "        self.take_step(action)\n",
        "\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        # Get human preferences\n",
        "        end_effector_1_velocity = np.linalg.norm(self.robot_1.get_velocity(self.robot_1.right_end_effector))\n",
        "        end_effector_2_velocity = np.linalg.norm(self.robot_2.get_velocity(self.robot_2.right_end_effector))\n",
        "        end_effector_velocity = (end_effector_1_velocity + end_effector_2_velocity)/2\n",
        "\n",
        "        total_force_on_human = np.sum(self.robot_1.get_contact_points(self.human)[-1])\n",
        "        total_force_on_human += np.sum(self.robot_2.get_contact_points(self.human)[-1])\n",
        "        preferences_score = self.human_preferences(end_effector_velocity=end_effector_velocity, total_force_on_human=total_force_on_human)\n",
        "\n",
        "        # Define our reward function\n",
        "        end_effector_1_pos, end_effector_1_orient = self.robot_1.get_pos_orient(self.robot_1.right_end_effector)\n",
        "        reward_1_distance_mouth_target = -np.linalg.norm(self.target_pos - end_effector_1_pos)\n",
        "\n",
        "        end_effector_2_pos, end_effector_2_orient = self.robot_2.get_pos_orient(self.robot_2.right_end_effector)\n",
        "        reward_2_distance_mouth_target = -np.linalg.norm(self.target_pos - end_effector_2_pos)\n",
        "\n",
        "        reward_distance_mouth_target = (reward_1_distance_mouth_target + reward_2_distance_mouth_target)/2\n",
        "        #end_effector_pos, end_effector_orient = self.robot.get_pos_orient(self.robot.right_end_effector)\n",
        "        #reward_distance_mouth_target = -np.linalg.norm(self.target_pos - end_effector_pos) # Penalize robot for distance between the end effector and human mouth.\n",
        "        reward_action = -np.linalg.norm(action) # Penalize actions\n",
        "        reward = self.config('distance_weight')*reward_distance_mouth_target + self.config('action_weight')*reward_action + preferences_score\n",
        "\n",
        "        info = {'total_force_on_human': total_force_on_human}\n",
        "        done = self.iteration >= 200\n",
        "\n",
        "        if not self.human.controllable:\n",
        "            return obs, reward, done, info\n",
        "        else:\n",
        "            # Co-optimization with both human and robot controllable\n",
        "            return obs, {'robot': reward, 'human': reward}, {'robot': done, 'human': done, '__all__': done}, {'robot': info, 'human': info}\n",
        "\n",
        "    def init_env_variables(self, reset=False):\n",
        "        if len(self.action_space.low) <= 1 or reset:\n",
        "            obs_len = len(self._get_obs())\n",
        "            self.observation_space.__init__(low=-np.ones(obs_len, dtype=np.float32)*1000000000, high=np.ones(obs_len, dtype=np.float32)*1000000000, dtype=np.float32)\n",
        "            self.update_action_space()\n",
        "            # Define action/obs lengths\n",
        "            self.action_robot_len = len(self.robot_1.controllable_joint_indices)+len(self.robot_2.controllable_joint_indices)\n",
        "            self.action_human_len = len(self.human.controllable_joint_indices) if self.human.controllable else 0\n",
        "            self.obs_robot_len = len(self._get_obs('robot'))\n",
        "            self.obs_human_len = len(self._get_obs('human'))\n",
        "            self.action_space_robot = spaces.Box(low=np.array([-1.0]*self.action_robot_len, dtype=np.float32), high=np.array([1.0]*self.action_robot_len, dtype=np.float32), dtype=np.float32)\n",
        "            self.action_space_human = spaces.Box(low=np.array([-1.0]*self.action_human_len, dtype=np.float32), high=np.array([1.0]*self.action_human_len, dtype=np.float32), dtype=np.float32)\n",
        "            self.observation_space_robot = spaces.Box(low=np.array([-1000000000.0]*self.obs_robot_len, dtype=np.float32), high=np.array([1000000000.0]*self.obs_robot_len, dtype=np.float32), dtype=np.float32)\n",
        "            self.observation_space_human = spaces.Box(low=np.array([-1000000000.0]*self.obs_human_len, dtype=np.float32), high=np.array([1000000000.0]*self.obs_human_len, dtype=np.float32), dtype=np.float32)\n",
        "\n",
        "    def _get_robot_obs(self,index=1):\n",
        "        if index == 1:\n",
        "            self_robot = self.robot_1\n",
        "            peer_robot = self.robot_2\n",
        "        else:\n",
        "            self_robot = self.robot_2\n",
        "            peer_robot = self.robot_1\n",
        "        \n",
        "        self_end_effector_pos, self_end_effector_orient = self_robot.get_pos_orient(self_robot.right_end_effector)\n",
        "        self_end_effector_pos_real, self_end_effector_orient_real = self_robot.convert_to_realworld(self_end_effector_pos, self_end_effector_orient)\n",
        "        self_robot_joint_angles = self_robot.get_joint_angles(self_robot.controllable_joint_indices)\n",
        "        # Fix joint angles to be in [-pi, pi]\n",
        "        self_robot_joint_angles = (np.array(self_robot_joint_angles) + np.pi) % (2*np.pi) - np.pi\n",
        "        if self_robot.mobile:\n",
        "            # Don't include joint angles for the wheels\n",
        "            self_robot_joint_angles = self_robot_joint_angles[len(self_robot.wheel_joint_indices):]\n",
        "        head_pos, head_orient = self.human.get_pos_orient(self.human.head)\n",
        "        head_pos_real, head_orient_real = self_robot.convert_to_realworld(head_pos, head_orient)\n",
        "        target_pos_real, _ = self_robot.convert_to_realworld(self.target_pos)\n",
        "\n",
        "        peer_end_effector_pos,pee_end_effector_orient = peer_robot.get_pos_orient(peer_robot.right_end_effector)\n",
        "        peer_end_effector_pos_real, peer_end_effector_orient_real = self_robot.convert_to_realworld(peer_end_effector_pos, pee_end_effector_orient)\n",
        "        # Define the robot observation\n",
        "        robot_obs = np.concatenate([self_end_effector_pos_real, self_end_effector_orient_real, \n",
        "                                    self_end_effector_pos_real - target_pos_real, self_robot_joint_angles, \n",
        "                                    head_pos_real, head_orient_real]).ravel()\n",
        "        return robot_obs\n",
        "\n",
        "    def _get_human_obs(self):\n",
        "        human_obs = None\n",
        "        if self.human.controllable:\n",
        "            human_joint_angles = self.human.get_joint_angles(self.human.controllable_joint_indices)\n",
        "            head_pos, head_orient = self.human.get_pos_orient(self.human.head)\n",
        "            target_pos_human, _ = self.human.convert_to_realworld(self.target_pos)\n",
        "            head_pos_human, head_orient_human = self.human.convert_to_realworld(head_pos, head_orient)\n",
        "            # Convert robot_1 pos/orient from global coordinates to human-centric coordinate frame\n",
        "            end_effector_pos_1, end_effector_orient_1 = self.robot_1.get_pos_orient(self.robot_1.right_end_effector)\n",
        "            end_effector_pos_1_human, end_effector_orient_1_human = self.human.convert_to_realworld(end_effector_pos_1, end_effector_orient_1)\n",
        "            # Convert robot_2 pos/orient from global coordinates to human-centric coordinate frame\n",
        "            end_effector_pos_2, end_effector_orient_2 = self.robot_2.get_pos_orient(self.robot_2.right_end_effector)\n",
        "            end_effector_pos_2_human, end_effector_orient_2_human = self.human.convert_to_realworld(end_effector_pos_2, end_effector_orient_2)\n",
        "            \n",
        "            total_force_on_human = np.sum(self.robot_1.get_contact_points(self.human)[-1])\n",
        "            total_force_on_human += np.sum(self.robot_2.get_contact_points(self.human)[-1])\n",
        "            # Define the human observation\n",
        "            human_obs = np.concatenate([end_effector_pos_1_human, end_effector_orient_1_human, end_effector_pos_1_human - target_pos_human,\n",
        "                                        end_effector_pos_2_human, end_effector_orient_2_human, end_effector_pos_2_human - target_pos_human,\n",
        "                                        human_joint_angles, head_pos_human, head_orient_human, [total_force_on_human]]).ravel()\n",
        "        return human_obs\n",
        "\n",
        "    def _get_obs(self, agent=None):\n",
        "        robot_1_obs = self._get_robot_obs(index=1)\n",
        "        robot_2_obs = self._get_robot_obs(index=2)\n",
        "        robot_obs = np.concatenate((robot_1_obs,robot_2_obs))\n",
        "        #robot_obs = self._get_all_robot_obs()\n",
        "        if(agent == 'robot_1'):\n",
        "            return robot_1_obs\n",
        "        if(agent =='robot_2'):\n",
        "            return robot_2_obs\n",
        "        if agent == 'robot':\n",
        "            return robot_obs\n",
        "        if self.human.controllable:\n",
        "            human_obs = self._get_human_obs()\n",
        "            if agent == 'human':\n",
        "                return human_obs\n",
        "            # Co-optimization with both human and robot controllable\n",
        "            return {'robot': robot_obs, 'human': human_obs}\n",
        "        return robot_obs\n",
        "\n",
        "    def reset(self):\n",
        "        super(ReachingMultiRobotEnv, self).reset()\n",
        "        self.build_assistive_env('wheelchair')\n",
        "        if self.robot.wheelchair_mounted:\n",
        "            wheelchair_pos, wheelchair_orient = self.furniture.get_base_pos_orient()\n",
        "            self.robot.set_base_pos_orient(wheelchair_pos + np.array(self.robot.toc_base_pos_offset[self.task]), [0, 0, -np.pi/2.0])\n",
        "        self.robot_1 = self.robot\n",
        "                \n",
        "        # Initialize the starting human pose\n",
        "        joints_positions = [(self.human.j_right_elbow, -90), (self.human.j_left_elbow, -90), (self.human.j_right_hip_x, -90),\n",
        "                            (self.human.j_right_knee, 80), (self.human.j_left_hip_x, -90), (self.human.j_left_knee, 80)]\n",
        "        joints_positions += [(self.human.j_head_x, self.np_random.uniform(-30, 30)), (self.human.j_head_y, self.np_random.uniform(-30, 30)),\n",
        "                             (self.human.j_head_z, self.np_random.uniform(-30, 30))]\n",
        "        self.human.setup_joints(joints_positions, use_static_joints=True, reactive_force=None)\n",
        "\n",
        "        self.generate_targets()\n",
        "\n",
        "        # Change camera pos/angle\n",
        "        p.resetDebugVisualizerCamera(cameraDistance=1.10, cameraYaw=40, cameraPitch=-45, cameraTargetPosition=[-0.2, 0, 0.75], physicsClientId=self.id)\n",
        "\n",
        "        # Perform base pose optimization\n",
        "        target_ee_pos = np.array([-0.15, -0.65, 1.15]) + self.np_random.uniform(-0.05, 0.05, size=3)\n",
        "        target_ee_orient = self.get_quaternion(self.robot.toc_ee_orient_rpy[self.task])\n",
        "        self.init_robot_pose(target_ee_pos, target_ee_orient, [(target_ee_pos, target_ee_orient), (self.target_pos, None)], \n",
        "                             [(self.target_pos, target_ee_orient)], arm='right', tools=[], collision_objects=[self.human, self.furniture])\n",
        "\n",
        "        if not self.robot.mobile:\n",
        "            self.robot.set_gravity(0, 0, 0)\n",
        "        self.human.set_gravity(0, 0, 0)\n",
        "        #reset second robot\n",
        "        self.robot_2 = self.create_robot(self.robot_1.__class__)\n",
        "        self.robot_2.set_gravity(0, 0, 0)\n",
        "        pos1, orient1 = self.robot_1.get_base_pos_orient()\n",
        "        self.robot_2.set_base_pos_orient(pos1 + np.array([2.0, 0, 0.0]), [0, 0, np.pi])\n",
        "        if not self.robot_2.mobile:\n",
        "            self.robot_2.set_gravity(0, 0, 0)\n",
        "        \n",
        "        #set default robot back to robot 1 \n",
        "        self.robot = self.robot_1        \n",
        "        # Enable rendering\n",
        "        p.configureDebugVisualizer(p.COV_ENABLE_RENDERING, 1, physicsClientId=self.id)\n",
        "\n",
        "        self.init_env_variables()\n",
        "        return self._get_obs()\n",
        "\n",
        "    def generate_targets(self):\n",
        "        # Set target on mouth\n",
        "        self.mouth_pos = [0, -0.11, 0.03] if self.human.gender == 'male' else [0, -0.1, 0.03]\n",
        "        head_pos, head_orient = self.human.get_pos_orient(self.human.head)\n",
        "        target_pos, target_orient = p.multiplyTransforms(head_pos, head_orient, self.mouth_pos, [0, 0, 0, 1], physicsClientId=self.id)\n",
        "        self.target = self.create_sphere(radius=0.01, mass=0.0, pos=target_pos, collision=False, rgba=[0, 1, 0, 1])\n",
        "        self.update_targets()\n",
        "\n",
        "    def update_targets(self):\n",
        "        # Update target position on mouth\n",
        "        head_pos, head_orient = self.human.get_pos_orient(self.human.head)\n",
        "        target_pos, target_orient = p.multiplyTransforms(head_pos, head_orient, self.mouth_pos, [0, 0, 0, 1], physicsClientId=self.id)\n",
        "        self.target_pos = np.array(target_pos)\n",
        "        self.target.set_base_pos_orient(self.target_pos, [0, 0, 0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xEOxuLFXtNS"
      },
      "source": [
        "## Create a version of this environment for each robot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ictnUVIPXF8z"
      },
      "outputs": [],
      "source": [
        "from assistive_gym.envs.agents import pr2, baxter, sawyer, jaco, stretch, panda, human\n",
        "from assistive_gym.envs.agents.sawyer import Sawyer\n",
        "from assistive_gym.envs.agents.jaco import Jaco\n",
        "from assistive_gym.envs.agents.pr2 import PR2\n",
        "from assistive_gym.envs.agents.baxter import Baxter\n",
        "from assistive_gym.envs.agents.panda import Panda\n",
        "from assistive_gym.envs.agents.stretch import Stretch\n",
        "from assistive_gym.envs.agents.human import Human\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "from ray.tune.registry import register_env\n",
        "import assistive_gym.envs\n",
        "\n",
        "robot_arm = 'right'\n",
        "human_controllable_joint_indices = human.head_joints\n",
        "class ReachingMultiSawyerEnv(ReachingMultiRobotEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiSawyerEnv, self).__init__(robot_1=Sawyer(robot_arm), \n",
        "                                                    human=Human(human_controllable_joint_indices, controllable=False),\n",
        "                                                    robot_2=Sawyer(robot_arm))\n",
        "# Connect the environment to Assistive Gym\n",
        "assistive_gym.envs.ReachingMultiSawyerEnv = ReachingMultiSawyerEnv\n",
        "register_env('assistive_gym:ReachingMultiSawyer-v1', lambda config: ReachingMultiSawyerEnv())\n",
        "\n",
        "class ReachingMultiJacoEnv(ReachingMultiRobotEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiJacoEnv, self).__init__(robot_1=Jaco(robot_arm),\n",
        "                                                 human=Human(human_controllable_joint_indices, controllable=False),\n",
        "                                                 robot_2=Jaco(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiJacoEnv = ReachingMultiJacoEnv\n",
        "register_env('assistive_gym:ReachingMultiJaco-v1', lambda config: ReachingMultiJacoEnv())\n",
        "\n",
        "class ReachingMultiPR2Env(ReachingMultiRobotEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiPR2Env, self).__init__(robot_1=PR2(robot_arm), \n",
        "                                                human=Human(human_controllable_joint_indices, controllable=False),\n",
        "                                                robot_2=PR2(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiPR2Env = ReachingMultiPR2Env\n",
        "register_env('assistive_gym:ReachingMultiPR2-v1', lambda config: ReachingMultiPR2Env())\n",
        "\n",
        "class ReachingMultiBaxterEnv(ReachingMultiRobotEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiBaxterEnv, self).__init__(robot_1=Baxter(robot_arm), \n",
        "                                                    human=Human(human_controllable_joint_indices, controllable=False),\n",
        "                                                    robot_2=Baxter(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiBaxterEnv = ReachingMultiBaxterEnv\n",
        "register_env('assistive_gym:ReachingMultiBaxter-v1', lambda config: ReachingMultiBaxterEnv())\n",
        "\n",
        "class ReachingMultiPandaEnv(ReachingMultiRobotEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiPandaEnv, self).__init__(robot_1=Panda(robot_arm), \n",
        "                                                    human=Human(human_controllable_joint_indices, controllable=False),\n",
        "                                                    robot_2=Panda(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiPandaEnv = ReachingMultiPandaEnv\n",
        "register_env('assistive_gym:ReachingMultiPanda-v1', lambda config: ReachingMultiPandaEnv())\n",
        "\n",
        "class ReachingMultiStretchEnv(ReachingMultiRobotEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiStretchEnv, self).__init__(robot_1=Stretch(robot_arm), \n",
        "                                                    human=Human(human_controllable_joint_indices, controllable=False),\n",
        "                                                    robot_2=Stretch(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiStretchrEnv = ReachingMultiStretchEnv\n",
        "register_env('assistive_gym:ReachingMultiStretch-v1', lambda config: ReachingMultiStretchEnv())\n",
        "\n",
        "\n",
        "class ReachingMultiSawyerHumanEnv(ReachingMultiRobotEnv, MultiAgentEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiSawyerHumanEnv, self).__init__(robot_1=Sawyer(robot_arm), \n",
        "                                                        human=Human(human_controllable_joint_indices, controllable=True),\n",
        "                                                        robot_2=Sawyer(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiSawyerHumanEnv = ReachingMultiSawyerHumanEnv\n",
        "# Register the collaborative robot-human environment with RLlib\n",
        "register_env('assistive_gym:ReachingMultiSawyerHuman-v1', lambda config: ReachingMultiSawyerHumanEnv())\n",
        "\n",
        "class ReachingMultiJacoHumanEnv(ReachingMultiRobotEnv, MultiAgentEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiJacoHumanEnv, self).__init__(robot_1=Jaco(robot_arm), \n",
        "                                                        human=Human(human_controllable_joint_indices, controllable=True),\n",
        "                                                        robot_2=Jaco(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiJacoHumanEnv = ReachingMultiJacoHumanEnv\n",
        "register_env('assistive_gym:ReachingMultiJacoHuman-v1', lambda config: ReachingMultiJacoHumanEnv())\n",
        "\n",
        "class ReachingMultiPR2HumanEnv(ReachingMultiRobotEnv, MultiAgentEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiPR2HumanEnv, self).__init__(robot_1=PR2(robot_arm), \n",
        "                                                    human=Human(human_controllable_joint_indices, controllable=True),\n",
        "                                                    robot_2=PR2(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiPR2HumanEnv = ReachingMultiPR2HumanEnv\n",
        "# Register the collaborative robot-human environment with RLlib\n",
        "register_env('assistive_gym:ReachingMultiPR2Human-v1', lambda config: ReachingMultiPR2HumanEnv())\n",
        "\n",
        "class ReachingMultiBaxterHumanEnv(ReachingMultiRobotEnv, MultiAgentEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiBaxterHumanEnv, self).__init__(robot_1=Baxter(robot_arm), \n",
        "                                                        human=Human(human_controllable_joint_indices, controllable=True),\n",
        "                                                        robot_2=Baxter(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiBaxterHumanEnv = ReachingMultiBaxterHumanEnv\n",
        "# Register the collaborative robot-human environment with RLlib\n",
        "register_env('assistive_gym:ReachingMultiBaxterHuman-v1', lambda config: ReachingMultiBaxterHumanEnv())\n",
        "\n",
        "class ReachingMultiPandaHumanEnv(ReachingMultiRobotEnv, MultiAgentEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiPandaHumanEnv, self).__init__(robot_1=Panda(robot_arm), \n",
        "                                                        human=Human(human_controllable_joint_indices, controllable=True),\n",
        "                                                        robot_2=Panda(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiPandaHumanEnv = ReachingMultiPandaHumanEnv\n",
        "# Register the collaborative robot-human environment with RLlib\n",
        "register_env('assistive_gym:ReachingMultiPandaHuman-v1', lambda config: ReachingMultiPandaHumanEnv())\n",
        "\n",
        "\n",
        "class ReachingMultiStretchHumanEnv(ReachingMultiRobotEnv, MultiAgentEnv):\n",
        "    def __init__(self):\n",
        "        super(ReachingMultiStretchHumanEnv, self).__init__(robot_1=Stretch(robot_arm), \n",
        "                                                            human=Human(human_controllable_joint_indices, controllable=True),\n",
        "                                                            robot_2=Stretch(robot_arm))\n",
        "assistive_gym.envs.ReachingMultiStretchHumanEnv = ReachingMultiStretchHumanEnv\n",
        "# Register the collaborative robot-human environment with RLlib\n",
        "register_env('assistive_gym:ReachingMultiStretchHuman-v1', lambda config: ReachingMultiStretchHumanEnv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90dW3VReX398"
      },
      "source": [
        "## Connect the environments to OpenAI Gym\n",
        "Only the non-collaborative environments where the human holds a static pose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdKAFgxWZkav"
      },
      "outputs": [],
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "for robot in ['Sawyer', 'Jaco','PR2','Baxter','Panda','Stretch']:\n",
        "    register(\n",
        "        id='ReachingMulti%s-v1' % robot,\n",
        "        entry_point='assistive_gym.envs:ReachingMulti%sEnv' % robot,\n",
        "        max_episode_steps=200,\n",
        "    )\n",
        "    register(\n",
        "        id='ReachingMulti%sHuman-v1' % robot,\n",
        "        entry_point='assistive_gym.envs:ReachingMulti%sHumanEnv' % robot,\n",
        "        max_episode_steps=200,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from assistive_gym.learn import train, render_policy, evaluate_policy\n",
        "env_name = 'ReachingMultiSawyer-v1'\n",
        "algo = 'ppo'\n",
        "policy_path = train(env_name, algo, timesteps_total=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q2mdQZLcYrl"
      },
      "source": [
        "### Rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfkYLJh7dHXJ",
        "outputId": "0f7866cc-0046-4e5c-b0f4-fc020c116ee3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('assistive_gym:'+env_name)\n",
        "env.setup_camera(camera_eye=[0.5, -0.75, 1.5], camera_target=[-0.2, 0, 0.75], fov=60, camera_width=1920//4, camera_height=1080//4)\n",
        "filename = render_policy(env, env_name, algo, policy_path, colab=True, seed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "7iSr25tScvjS",
        "outputId": "b704261c-7ace-4a17-91c8-d2de77094613"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assistive_Gym_New_Env.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('sim_py37')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "54dd62402dfd40248cbbb50e5b188b622ab13fc630784af8a94b97379b5d537c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
